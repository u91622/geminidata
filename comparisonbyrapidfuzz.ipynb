{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqyCqwuKqEsD",
        "outputId": "70dae555-5309-435c-ac75-1c5e9e7f0ff1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rapidfuzz\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/3.1 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m2.6/3.1 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz\n",
            "Successfully installed rapidfuzz-3.13.0\n",
            "Mounted at /content/drive\n",
            "檔案存在: /content/drive/My Drive/Datasets/largeds1.csv\n",
            "檔案存在: /content/drive/My Drive/Datasets/largeds2.csv\n",
            "檔案存在: /content/drive/My Drive/Datasets/largeds3.csv\n",
            "檔案存在: /content/drive/My Drive/Datasets/largeds4.csv\n",
            "檔案存在: /content/drive/My Drive/Datasets/largeds5.csv\n",
            "檔案存在: /content/drive/My Drive/Datasets/largeds6.csv\n",
            "檔案存在: /content/drive/My Drive/Datasets/smallds.csv\n",
            "合併後大資料集行數：4937032\n",
            "小資料集行數：5831\n",
            "大資料集欄位： ['統一編號', '公司名稱', '產業類別']\n",
            "小資料集欄位： ['項目名稱', '選舉名稱', '申報序號／年度', '交易日期', '收支科目', '公司名稱', '身分證／統一編號', '收入金額', '支出金額', '支出用途', '金錢類', '地址', '聯絡電話', '捐贈方式', '存入專戶日期', '返還/繳庫', '應揭露之支出對象', '支出對象之內部人員姓名', '支出對象之內部人員職稱', '政黨之內部人員姓名', '政黨之內部人員職稱', '關係', '更正註記', '資料更正日期']\n",
            "去重後大資料集行數：907312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "處理批次 1: 100%|██████████| 1000/1000 [02:53<00:00,  5.76it/s]\n",
            "處理批次 2: 100%|██████████| 1000/1000 [02:54<00:00,  5.74it/s]\n",
            "處理批次 3: 100%|██████████| 1000/1000 [02:53<00:00,  5.78it/s]\n",
            "處理批次 4: 100%|██████████| 1000/1000 [02:56<00:00,  5.66it/s]\n",
            "處理批次 5: 100%|██████████| 1000/1000 [02:52<00:00,  5.81it/s]\n",
            "處理批次 6: 100%|██████████| 831/831 [02:16<00:00,  6.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "已完成模糊比對，小資料集已補充產業類別，儲存為 /content/drive/My Drive/Datasets/merged_dataset_20250505_041910.csv\n",
            "警告：有 1057 筆資料的產業類別缺失，可能是公司名稱在大資料集中無匹配或相似度低於 85。\n",
            "缺失產業類別的資料已儲存為 /content/drive/My Drive/Datasets/missing_industry_20250505_041910.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install rapidfuzz\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "from rapidfuzz import fuzz  # 替代 fuzzywuzzy\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm  # 進度條，方便查看進度\n",
        "import numpy as np\n",
        "\n",
        "# 掛載 Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 設定檔案路徑\n",
        "base_dir = '/content/drive/My Drive/Datasets/'\n",
        "big_dataset_paths = [\n",
        "    base_dir + 'largeds1.csv',\n",
        "    base_dir + 'largeds2.csv',\n",
        "    base_dir + 'largeds3.csv',\n",
        "    base_dir + 'largeds4.csv',\n",
        "    base_dir + 'largeds5.csv',\n",
        "    base_dir + 'largeds6.csv'\n",
        "]\n",
        "small_dataset_path = base_dir + 'smallds.csv'\n",
        "output_dir = base_dir\n",
        "\n",
        "# 檢查檔案是否存在\n",
        "for path in big_dataset_paths + [small_dataset_path]:\n",
        "    if os.path.exists(path):\n",
        "        print(f\"檔案存在: {path}\")\n",
        "    else:\n",
        "        print(f\"檔案不存在: {path}\")\n",
        "\n",
        "# 確保輸出資料夾存在\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# 生成唯一的輸出檔案名稱（使用時間戳）\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "output_path = os.path.join(output_dir, f'merged_dataset_{timestamp}.csv')\n",
        "\n",
        "# 載入小資料集\n",
        "try:\n",
        "    small_dataset = pd.read_csv(small_dataset_path, encoding='utf-8', dtype={0: str}, low_memory=False)\n",
        "except UnicodeDecodeError:\n",
        "    print(\"小資料集編碼錯誤，嘗試 utf-8-sig\")\n",
        "    small_dataset = pd.read_csv(small_dataset_path, encoding='utf-8-sig', dtype={0: str}, low_memory=False)\n",
        "\n",
        "# 初始化合併後的大資料集\n",
        "big_dataset_combined = pd.DataFrame()\n",
        "\n",
        "# 載入並合併所有大資料集\n",
        "for big_path in big_dataset_paths:\n",
        "    try:\n",
        "        big_dataset = pd.read_csv(big_path, encoding='utf-8', dtype={0: str}, low_memory=False)\n",
        "    except UnicodeDecodeError:\n",
        "        print(f\"大資料集 {big_path} 編碼錯誤，嘗試 utf-8-sig\")\n",
        "        big_dataset = pd.read_csv(big_path, encoding='utf-8-sig', dtype={0: str}, low_memory=False)\n",
        "    big_dataset_combined = pd.concat([big_dataset_combined, big_dataset], ignore_index=True)\n",
        "\n",
        "# 檢查資料集大小和欄位\n",
        "print(f\"合併後大資料集行數：{len(big_dataset_combined)}\")\n",
        "print(f\"小資料集行數：{len(small_dataset)}\")\n",
        "print(\"大資料集欄位：\", big_dataset_combined.columns.tolist())\n",
        "print(\"小資料集欄位：\", small_dataset.columns.tolist())\n",
        "\n",
        "# TODO: 根據列印的欄位名稱，更新以下變數\n",
        "company_name_col = '公司名稱'  # 替換為實際的公司名稱欄位\n",
        "industry_col = '產業類別'      # 替換為實際的產業類別欄位\n",
        "\n",
        "# 確保公司名稱為字串格式並去除多餘空格\n",
        "big_dataset_combined[company_name_col] = big_dataset_combined[company_name_col].astype(str).str.strip()\n",
        "small_dataset[company_name_col] = small_dataset[company_name_col].astype(str).str.strip()\n",
        "\n",
        "# 清理名稱：去除常見後綴以提高匹配率\n",
        "big_dataset_combined[company_name_col] = big_dataset_combined[company_name_col].str.replace(\n",
        "    r'股份有限公司|有限公司|股份公司|$$股$$|公司', '', regex=True).str.lower()\n",
        "small_dataset[company_name_col] = small_dataset[company_name_col].str.replace(\n",
        "    r'股份有限公司|有限公司|股份公司|$$股$$|公司', '', regex=True).str.lower()\n",
        "\n",
        "# 處理空值或無效名稱\n",
        "big_dataset_combined = big_dataset_combined[big_dataset_combined[company_name_col].notna() & (big_dataset_combined[company_name_col] != '')]\n",
        "small_dataset = small_dataset[small_dataset[company_name_col].notna() & (small_dataset[company_name_col] != '')]\n",
        "\n",
        "# 優化 1：去除大資料集中的重複公司名稱，保留第一筆產業類別\n",
        "big_dataset_combined = big_dataset_combined.drop_duplicates(subset=[company_name_col], keep='first')\n",
        "print(f\"去重後大資料集行數：{len(big_dataset_combined)}\")\n",
        "\n",
        "# 優化 2：根據公司名稱的首字進行分組，減少比對次數\n",
        "# 為公司名稱新增首字欄位\n",
        "big_dataset_combined['first_char'] = big_dataset_combined[company_name_col].str[0]\n",
        "small_dataset['first_char'] = small_dataset[company_name_col].str[0]\n",
        "\n",
        "# 模糊比對函數（使用 rapidfuzz 的 partial_ratio 加速）\n",
        "def find_best_match(name, name_list, min_score=85):\n",
        "    best_match = None\n",
        "    best_score = min_score\n",
        "    best_industry = None\n",
        "    for _, row in name_list.iterrows():\n",
        "        score = fuzz.partial_ratio(name, row[company_name_col])  # 使用 rapidfuzz 的 partial_ratio\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_match = row[company_name_col]\n",
        "            best_industry = row[industry_col]\n",
        "    return best_match, best_score, best_industry\n",
        "\n",
        "# 分批處理小資料集，並按首字分組比對\n",
        "batch_size = 1000\n",
        "results = []\n",
        "unique_first_chars = small_dataset['first_char'].unique()\n",
        "\n",
        "for start in range(0, len(small_dataset), batch_size):\n",
        "    batch = small_dataset.iloc[start:start + batch_size].copy()\n",
        "    batch['matched_name'] = None\n",
        "    batch['match_score'] = None\n",
        "    batch[industry_col] = None\n",
        "\n",
        "    # 對每個批次中的資料，根據首字過濾大資料集\n",
        "    for idx, row in tqdm(batch.iterrows(), total=len(batch), desc=f\"處理批次 {start//batch_size + 1}\"):\n",
        "        name = row[company_name_col]\n",
        "        first_char = row['first_char']\n",
        "        # 只比對相同首字的大資料集子集\n",
        "        filtered_big_list = big_dataset_combined[big_dataset_combined['first_char'] == first_char]\n",
        "        if not filtered_big_list.empty:\n",
        "            best_match, best_score, best_industry = find_best_match(name, filtered_big_list)\n",
        "        else:\n",
        "            best_match, best_score, best_industry = None, 0, None\n",
        "        batch.at[idx, 'matched_name'] = best_match\n",
        "        batch.at[idx, 'match_score'] = best_score\n",
        "        batch.at[idx, industry_col] = best_industry\n",
        "    results.append(batch)\n",
        "\n",
        "# 合併所有批次結果\n",
        "merged_dataset = pd.concat(results)\n",
        "\n",
        "# 移除臨時欄位（僅保留公司名稱和產業類別）\n",
        "merged_dataset = merged_dataset.drop(columns=['matched_name', 'match_score', 'first_char'])\n",
        "\n",
        "# 儲存結果，使用 utf-8-sig 編碼\n",
        "merged_dataset.to_csv(output_path, encoding='utf-8-sig', index=False)\n",
        "print(f\"已完成模糊比對，小資料集已補充產業類別，儲存為 {output_path}\")\n",
        "\n",
        "# 檢查缺失值\n",
        "min_score = 85  # 與 find_best_match 中定義的一致\n",
        "missing_count = merged_dataset[industry_col].isna().sum()\n",
        "if missing_count > 0:\n",
        "    print(f\"警告：有 {missing_count} 筆資料的產業類別缺失，可能是公司名稱在大資料集中無匹配或相似度低於 {min_score}。\")\n",
        "    missing_output_path = os.path.join(output_dir, f'missing_industry_{timestamp}.csv')\n",
        "    missing_data = merged_dataset[merged_dataset[industry_col].isna()]\n",
        "    missing_data.to_csv(missing_output_path, encoding='utf-8-sig', index=False)\n",
        "    print(f\"缺失產業類別的資料已儲存為 {missing_output_path}\")"
      ]
    }
  ]
}